default:
  data:
    overall_date_range:
      start: "2010-01"
      end: "2010-04"
    rolling_window:
      train_window_months: 1
      test_offset: 1
  features:
    base:
      - mom1m
      - mom6m
      - mom12m
      - retvol
      - bm
      - siccd
      
  preprocessing_pipeline:
    - drop_na
    - quantile_transform
    - encode_industry
    - drop_na
  model:
    name: mlpk  # assuming this maps to MLPKerasModel in your registry
    params:
      hidden_layer_sizes:
        - [512, 256, 128, 64]  # Larger model to benefit from A100 compute
      activation:
        - relu
      solver:
        - adam
      learning_rate_init:
        - 0.002  # Slightly higher learning rate to pair with large batch size
      alpha:
        - 0.0005  # Slightly reduced regularization
      max_iter:
        - 1000  # More epochs; early stopping is enabled anyway

experiment_2:
  data:
    overall_date_range:
      start: "2010-01"
      end: "2010-12"
    rolling_window:
      train_window_months: 1
      test_offset: 1
  features:
    base:
     - maxret          
     - mom12m           
     - mom1m           
     - mom36m           
     - mom6m            
     - ms              
     - mve              
     - mve_ia           
     - nanalyst         
     - nincr           
     - operprof        
     - orgcap           
     - pchcapx_ia       
     - pchcurrat       
     - pchdepr  
     - siccd  
  preprocessing_pipeline:
    - drop_na
    - encode_industry
    - quantile_transform
    - drop na
  model:
    name: mlpk
    params:
      hidden_layer_sizes: 
        - [[512, 256, 128, 64]]
      activation:
        - relu
      solver:
        - adam
      learning_rate_init:
        - 0.002  # Slightly higher learning rate to pair with large batch size
      alpha:
        - 0.0005  # Slightly reduced regularization
      max_iter:
        - 1000  # More epochs; early stopping is enabled anyway

