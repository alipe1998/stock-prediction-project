default:
  data:
    overall_date_range:
      start: "2010-01"
      end: "2021-12"
    rolling_window:
      train_window_months: 1
      test_offset: 1
  features:
    base:
      - mom1m
      - mom6m
      - mom12m
      - mom36m 
      - maxret
      - indmom
      - siccd
  preprocessing_pipeline:
    - drop_na
    - quantile_transform
    - encode_industry
    - drop_na
  model:
    name: mlpk  # assuming this maps to MLPKerasModel in your registry
    params:
      hidden_layer_sizes:
        - [512, 256, 128, 64]  # Larger model to benefit from A100 compute
      activation:
        - relu
      solver:
        - adam
      learning_rate_init:
        - 0.002  # Slightly higher learning rate to pair with large batch size
      alpha:
        - 0.0005  # Slightly reduced regularization
      max_iter:
        - 1000  # More epochs; early stopping is enabled anyway

# overrides on top of your default block
experiment_1:
  data:
    overall_date_range:
      start: "2010-01"
      end: "2021-12"
    rolling_window:
      train_window_months: 1
      test_offset: 1
  features:
    base:
      - mom1m
      - mom6m
      - mom12m
      - mom36m
      - maxret
      - indmom
      - siccd
  preprocessing_pipeline:
    - drop_na
    - quantile_transform
    - encode_industry
    - drop_na
  model:
    name: mlpk
    params:
      hidden_layer_sizes:
        - [128, 64]         # small network
      activation:
        - relu
      solver:
        - adam
      learning_rate_init:
        - 0.001             # standard LR
      alpha:
        - 0.001             # modest L2 to stabilize
      max_iter:
        - 500               # fewer epochsâ€”quick feedback

experiment_2:
  data:
    overall_date_range:
      start: "2010-01"
      end: "2021-12"
    rolling_window:
      train_window_months: 1
      test_offset: 1
  features:
    base:
      - turn
      - std_turn
      - dolvol
      - std_dolvol
      - ill
      - pricedelay
      - siccd
  preprocessing_pipeline:
    - drop_na
    - quantile_transform
    - encode_industry
    - drop_na
  model:
    name: mlpk
    params:
      hidden_layer_sizes:
        - [64, 32]           # shallow net for noisy signals
      activation:
        - tanh               # tanh can help center noisy inputs
      solver:
        - adam
      learning_rate_init:
        - 0.0005             # slower LR for stability on noisy data
      alpha:
        - 0.005              # stronger L2 to prevent overfit
      max_iter:
        - 700                # moderate epochs



